{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05112022_LSTM_TextGeneration.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6zhiiItYAfzU4P4VU4+Ne",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomasMendozaHN/ICDF_Class/blob/main/05112022_LSTM_TextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin by defining the LSTM model"
      ],
      "metadata": {
        "id": "FqsQVFXKcchT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7JTXZK0tb09q",
        "outputId": "25161eca-9a22-4053-bfcf-37c51b5d9e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c4504696734f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3\n",
        "\n",
        "        # Embedding is a layer that converts words into one-hot encodings\n",
        "        # For example:\n",
        "        # hello my name is Tomas = 00001 00010 00100 01000 1000\n",
        "        # As you can see, each unique word has it's own \"embedding\"\n",
        "        # in this case, the embedding is a one-hot encoded vector\n",
        "        # therefore, the moroe unique words you have, the longer each vector will be\n",
        "        # Also, this means you can't generate words that the LSTM has not seen\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test loading an online CSV with the dataset in it"
      ],
      "metadata": {
        "id": "wpaS5VuZe_Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "YYJWuR0_eqW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the entire text (all jokes) as a single string"
      ],
      "metadata": {
        "id": "sca8__zRuUOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_a_single_string = dataset['Joke'].str.cat(sep=' ')\n",
        "print(text_as_a_single_string)"
      ],
      "metadata": {
        "id": "B1q0fb4_f_wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate all words in that string"
      ],
      "metadata": {
        "id": "OPLdUPFiuY3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "individual_words = text_as_a_single_string.split(' ')\n",
        "print(individual_words)"
      ],
      "metadata": {
        "id": "EV71H8HngMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtain a list containing all words (without repetition) in decreasing order of frequency"
      ],
      "metadata": {
        "id": "0LQJYzuBucZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "unique_words = Counter(individual_words)\n",
        "unique_words_in_order_of_frequency = sorted(unique_words, key=unique_words.get, reverse=True)\n",
        "print(unique_words_in_order_of_frequency)\n",
        "print(len(unique_words_in_order_of_frequency))"
      ],
      "metadata": {
        "id": "xSA40ox3gl96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert each word to an integer (for embedding)"
      ],
      "metadata": {
        "id": "RCA-9CUcuk2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {index: word for index, word in enumerate(unique_words_in_order_of_frequency)}\n",
        "print(index_to_word)"
      ],
      "metadata": {
        "id": "XYrjxDPNhFeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert each integer (embedding) back into word"
      ],
      "metadata": {
        "id": "1BMAhncXu8WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word: index for index, word in enumerate(unique_words_in_order_of_frequency)}\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "id": "uL3qnNdJti4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert the entire string of jokes into integers (using the word_to_index dictionary)"
      ],
      "metadata": {
        "id": "4AmjjZD4vBAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_indexes = [word_to_index[w] for w in individual_words]\n",
        "print(words_indexes)"
      ],
      "metadata": {
        "id": "QJCBED8Bt7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now that we have seen how we need to prepare our data, we must create a Dataset function that will do this automatically"
      ],
      "metadata": {
        "id": "YXpM1fqtfF5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Remember: Every dataset function you create MUST have the following three methods:\n",
        "# 1. __init__      --> must read and prepare the dataset\n",
        "# 2. __len__       --> must return the number of datapoints in your entire dataset\n",
        "# 3. __getitem__   --> must return a batch of data\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequence_length):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.words = self.load_words()\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
        "        )\n",
        "\n",
        "    def load_words(self):\n",
        "        train_df = pd.read_csv(\"https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv\")\n",
        "        text = train_df['Joke'].str.cat(sep=' ')\n",
        "        return text.split(' ')\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "a00-oWpceu-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# initialize the Dataloader"
      ],
      "metadata": {
        "id": "_0UWt0G-vaTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 6\n",
        "dataset = Dataset(sequence_length=sequence_length)"
      ],
      "metadata": {
        "id": "SIKQGBJNvcOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the output of the dataloader"
      ],
      "metadata": {
        "id": "-HmVYVcsxGnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for a,b in dataset:\n",
        "  print(f\"originally, our dataset returns a = {a}, b = {b}\")\n",
        "  \n",
        "  # Converting both tensors into strings\n",
        "  a,b = a.numpy(), b.numpy()\n",
        "  a = [index_to_word[x] for x in a] \n",
        "  b = [index_to_word[x] for x in b]\n",
        "  \n",
        "  print(f\"converting these back into text, we have: a = {a}, b = {b}\")\n",
        "  break"
      ],
      "metadata": {
        "id": "m1NlDvC8w_6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the model"
      ],
      "metadata": {
        "id": "04dnaVUOxPu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(dataset)"
      ],
      "metadata": {
        "id": "-eYsfxjpwH5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin training!"
      ],
      "metadata": {
        "id": "GGKyr6cXxnR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 10\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "5sZmcFPm0OFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, model, max_epochs, sequence_length, batch_size):\n",
        "    model.train()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        state_h, state_c = model.init_state(sequence_length)\n",
        "\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"
      ],
      "metadata": {
        "id": "zYrFUZVefMS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataset, model, max_epochs=max_epochs, sequence_length=sequence_length, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "je_lMgQPvVMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(dataset, model, text, next_words=100):\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split(' ')\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "h8km4cogvk8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(dataset, model, text=\"Knock knock. Who's there?\", next_words=5)"
      ],
      "metadata": {
        "id": "6mpV1gTtvlUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The results are still pretty bad. You can always improve it by:\n",
        "\n",
        "\n",
        "1.   Clean up the data by removing non-letter characters.\n",
        "2.   Increase the model capacity by adding more Linear or LSTM layers.\n",
        "3.   Split the dataset into train, test, and validation sets."
      ],
      "metadata": {
        "id": "mfhKohbayiYD"
      }
    }
  ]
}