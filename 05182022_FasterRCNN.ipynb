{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05182022_FasterRCNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTPsxUgzfCRCf5xBQBGXBr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomasMendozaHN/ICDF_Class/blob/main/05182022_FasterRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get my Dataset"
      ],
      "metadata": {
        "id": "XzRYBL0rJxY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CssrtaolJXIq",
        "outputId": "2ec23ec2-00b8-49a0-c07f-2075d3272647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x \"/content/drive/MyDrive/MaskDataset__ByTomasMendoza.rar\" \"/content/sample_data\""
      ],
      "metadata": {
        "id": "ON0n8nvkJzp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that everything works!\n",
        "import os\n",
        "folder = \"/content/sample_data/MaskDataset__ByTomasMendoza\"\n",
        "for sfolder in os.listdir(folder):\n",
        "  print(sfolder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j5mJizOKnE2",
        "outputId": "567c6eac-a60c-4dac-fb77-64cac7011b24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n",
            "new_labels\n",
            "imgs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some utils used by training"
      ],
      "metadata": {
        "id": "bfQu0DC3NTF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def _flip_coco_person_keypoints(kps, width):\n",
        "    flip_inds = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n",
        "    flipped_data = kps[:, flip_inds]\n",
        "    flipped_data[..., 0] = width - flipped_data[..., 0]\n",
        "    # Maintain COCO convention that if visibility == 0, then x, y = 0\n",
        "    inds = flipped_data[..., 2] == 0\n",
        "    flipped_data[inds] = 0\n",
        "    return flipped_data\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = image.flip(-1)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "            target[\"boxes\"] = bbox\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = target[\"masks\"].flip(-1)\n",
        "            if \"keypoints\" in target:\n",
        "                keypoints = target[\"keypoints\"]\n",
        "                keypoints = _flip_coco_person_keypoints(keypoints, width)\n",
        "                target[\"keypoints\"] = keypoints\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "AmH_K89NNTRZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin importing libraries"
      ],
      "metadata": {
        "id": "57t-yJRAMJAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict"
      ],
      "metadata": {
        "id": "n5g8yIiyMY2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "import xmltodict\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "from torchvision.transforms import functional as F\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device = torch.device(\"cuda:0\")\n",
        "print('You will train using your ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j8_CdBaMIVq",
        "outputId": "cfd3f0b7-e50d-4369-d835-35a46b5e1281"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You will train using your  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceMask_Dataset(object):\n",
        "    def __init__(self, root, obj, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.img_path = os.path.join(root, 'imgs')\n",
        "        self.label_path = os.path.join(root, 'new_labels')\n",
        "        \n",
        "        # Read labels\n",
        "        self.labels = list(sorted(os.listdir(self.label_path)))\n",
        "        label_names = [x[:-4] for x in self.labels]\n",
        "        \n",
        "        #Read Images\n",
        "        self.imgs = list(sorted(os.listdir(self.img_path)))\n",
        "        \n",
        "        #Make both lists of images and labels match to avoid errors\n",
        "        self.imgs = [x for x in self.imgs if x[:-4] in label_names]\n",
        "        img_names = [x[:-4] for x in self.imgs]\n",
        "        self.labels = [x for x in self.labels if x[:-4] in img_names]\\\n",
        "        \n",
        "        if obj=='train':\n",
        "            self.imgs = self.imgs[5000:]\n",
        "            self.labels = self.labels[5000:]\n",
        "        elif obj=='test':\n",
        "            self.imgs = self.imgs[:5000]\n",
        "            self.labels = self.labels[:5000]\n",
        "        \n",
        "\n",
        "    #Generates the bounding box based on the coordinates given\n",
        "    def create_box_from_coordinates(self, coordinates):\n",
        "        box=[int(coordinates[0][1:]),\n",
        "             int(coordinates[1]),\n",
        "             int(coordinates[2]),\n",
        "             int(coordinates[3][:-1])]\n",
        "        return [box]\n",
        "    \n",
        "\n",
        "    #Extracts the label and bounding box\n",
        "    def read_label(self, path):\n",
        "        #Read the xml file with the bounding boxes\n",
        "        file_object  = open(path, \"r\")\n",
        "        data = file_object.readlines()\n",
        "        data = data[0].split(',')[-6:-1]\n",
        "        \n",
        "        label, boxes = data[0], data[1:]\n",
        "        label = int(label)\n",
        "        if label == 0: #if NOT wearing mask, change to label 3\n",
        "            label = 3\n",
        "        box = self.create_box_from_coordinates(boxes)\n",
        "        return [label], box\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "#         print('idx: ', idx)\n",
        "        \n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.img_path, self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        label_path = os.path.join(self.label_path, self.labels[idx])\n",
        "        labels, boxes = self.read_label(label_path)\n",
        "        \n",
        "        #Calculate number of objects in image\n",
        "        n_objects=1\n",
        "        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        \n",
        "        #Calculate bounding box area\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "yZ1JQR8PLwi4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define function that converts pillow images into tensors"
      ],
      "metadata": {
        "id": "F9tq8Vr6O6Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms a pillow image to tensor \n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(ToTensor())\n",
        "    if train:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "    return Compose(transforms)"
      ],
      "metadata": {
        "id": "OFhJgqw9OzNf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster RCNN Parameters!!!"
      ],
      "metadata": {
        "id": "Xf7bsafUPKAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probability threshold for your Non-Maxima Supression\n",
        "box_nms_thresh = 0.5\n",
        "\n",
        "# 0 = No Mask\n",
        "# 1 = Incorrect Mask\n",
        "# 2 = With Mask\n",
        "num_classes = 3  "
      ],
      "metadata": {
        "id": "bagNa-gXPHf9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare your faster-RCNN backbone (pre-trained network)"
      ],
      "metadata": {
        "id": "0iKjGJz1Pl_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your must first load the previously trained FasterRCNN model from Pytorch\n",
        "# first in order to fine-tune it. \n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
        "                                                pretrained=True, \n",
        "                                                box_nms_thresh=box_nms_thresh\n",
        "                                                )\n",
        "\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)   \n",
        "\n",
        "print(model.roi_heads.box_predictor)"
      ],
      "metadata": {
        "id": "ogZj86x0POAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}