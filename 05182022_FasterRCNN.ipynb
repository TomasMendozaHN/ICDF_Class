{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05182022_FasterRCNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVFMEwL0d+tJvnnjiYWebu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ef3291b55d44b13b6e03c2ce4885818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93104761bb5c47c69318c9ed4c576d10",
              "IPY_MODEL_119e5bce567f4b1c8a7f579b60c3cfb9",
              "IPY_MODEL_e9fa9f6af2cd40148c506be7d46c082d"
            ],
            "layout": "IPY_MODEL_0880de47887a4245ac374243b2b56a5e"
          }
        },
        "93104761bb5c47c69318c9ed4c576d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbfad41e268e4b95bed2e1ad3c3c0386",
            "placeholder": "​",
            "style": "IPY_MODEL_cb4eb445404e41a88de930b081c780a2",
            "value": "100%"
          }
        },
        "119e5bce567f4b1c8a7f579b60c3cfb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a38e7a8c2af247ce8b0b93b21cb4e151",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11abce6e7ee64c898cf0357fd194522f",
            "value": 167502836
          }
        },
        "e9fa9f6af2cd40148c506be7d46c082d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_436d0ab5e4774b04a5a1b9fb7a55eafd",
            "placeholder": "​",
            "style": "IPY_MODEL_a8fc4784ba6544348d10b2678a08bc90",
            "value": " 160M/160M [00:03&lt;00:00, 52.7MB/s]"
          }
        },
        "0880de47887a4245ac374243b2b56a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbfad41e268e4b95bed2e1ad3c3c0386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb4eb445404e41a88de930b081c780a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a38e7a8c2af247ce8b0b93b21cb4e151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11abce6e7ee64c898cf0357fd194522f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "436d0ab5e4774b04a5a1b9fb7a55eafd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fc4784ba6544348d10b2678a08bc90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomasMendozaHN/ICDF_Class/blob/main/05182022_FasterRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get my Dataset"
      ],
      "metadata": {
        "id": "XzRYBL0rJxY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CssrtaolJXIq",
        "outputId": "62c59f9c-0cd0-4650-ac30-96b76eead702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x \"/content/drive/MyDrive/MaskDataset__ByTomasMendoza.rar\" \"/content/sample_data\"  # unrar \"from\" \"to\""
      ],
      "metadata": {
        "id": "ON0n8nvkJzp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that I can see the folders in!\n",
        "import os\n",
        "folder = \"/content/sample_data/MaskDataset__ByTomasMendoza\"\n",
        "for sfolder in os.listdir(folder):\n",
        "  print(sfolder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j5mJizOKnE2",
        "outputId": "0380448e-008a-496b-e92b-767a178b64b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n",
            "new_labels\n",
            "imgs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some utils used by training (created by PyTorch, not me, so just copy them)\n"
      ],
      "metadata": {
        "id": "bfQu0DC3NTF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import pycocotools.mask as mask_util"
      ],
      "metadata": {
        "id": "_1QQlcNPuFbd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types\n",
        "\n",
        "\n",
        "class FilterAndRemapCocoCategories(object):\n",
        "    def __init__(self, categories, remap=True):\n",
        "        self.categories = categories\n",
        "        self.remap = remap\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        anno = target[\"annotations\"]\n",
        "        anno = [obj for obj in anno if obj[\"category_id\"] in self.categories]\n",
        "        if not self.remap:\n",
        "            target[\"annotations\"] = anno\n",
        "            return image, target\n",
        "        anno = copy.deepcopy(anno)\n",
        "        for obj in anno:\n",
        "            obj[\"category_id\"] = self.categories.index(obj[\"category_id\"])\n",
        "        target[\"annotations\"] = anno\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask(object):\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if obj['iscrowd'] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "        masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] for obj in anno])\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def _coco_remove_images_without_annotations(dataset, cat_list=None):\n",
        "    def _has_only_empty_bbox(anno):\n",
        "        return all(any(o <= 1 for o in obj[\"bbox\"][2:]) for obj in anno)\n",
        "\n",
        "    def _count_visible_keypoints(anno):\n",
        "        return sum(sum(1 for v in ann[\"keypoints\"][2::3] if v > 0) for ann in anno)\n",
        "\n",
        "    min_keypoints_per_image = 10\n",
        "\n",
        "    def _has_valid_annotation(anno):\n",
        "        # if it's empty, there is no annotation\n",
        "        if len(anno) == 0:\n",
        "            return False\n",
        "        # if all boxes have close to zero area, there is no annotation\n",
        "        if _has_only_empty_bbox(anno):\n",
        "            return False\n",
        "        # keypoints task have a slight different critera for considering\n",
        "        # if an annotation is valid\n",
        "        if \"keypoints\" not in anno[0]:\n",
        "            return True\n",
        "        # for keypoint detection tasks, only consider valid images those\n",
        "        # containing at least min_keypoints_per_image\n",
        "        if _count_visible_keypoints(anno) >= min_keypoints_per_image:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    assert isinstance(dataset, torchvision.datasets.CocoDetection)\n",
        "    ids = []\n",
        "    for ds_idx, img_id in enumerate(dataset.ids):\n",
        "        ann_ids = dataset.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
        "        anno = dataset.coco.loadAnns(ann_ids)\n",
        "        if cat_list:\n",
        "            anno = [obj for obj in anno if obj[\"category_id\"] in cat_list]\n",
        "        if _has_valid_annotation(anno):\n",
        "            ids.append(ds_idx)\n",
        "\n",
        "    dataset = torch.utils.data.Subset(dataset, ids)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def convert_to_coco_api(ds):\n",
        "    coco_ds = COCO()\n",
        "    # annotation IDs need to start at 1, not 0, see torchvision issue #1530\n",
        "    ann_id = 1\n",
        "    dataset = {'images': [], 'categories': [], 'annotations': []}\n",
        "    categories = set()\n",
        "    for img_idx in range(len(ds)):\n",
        "        # find better way to get target\n",
        "        # targets = ds.get_annotations(img_idx)\n",
        "        img, targets = ds[img_idx]\n",
        "        image_id = targets[\"image_id\"].item()\n",
        "        img_dict = {}\n",
        "        img_dict['id'] = image_id\n",
        "        img_dict['height'] = img.shape[-2]\n",
        "        img_dict['width'] = img.shape[-1]\n",
        "        dataset['images'].append(img_dict)\n",
        "        bboxes = targets[\"boxes\"]\n",
        "        bboxes[:, 2:] -= bboxes[:, :2]\n",
        "        bboxes = bboxes.tolist()\n",
        "        labels = targets['labels'].tolist()\n",
        "        areas = targets['area'].tolist()\n",
        "        iscrowd = targets['iscrowd'].tolist()\n",
        "        if 'masks' in targets:\n",
        "            masks = targets['masks']\n",
        "            # make masks Fortran contiguous for coco_mask\n",
        "            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n",
        "        if 'keypoints' in targets:\n",
        "            keypoints = targets['keypoints']\n",
        "            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n",
        "        num_objs = len(bboxes)\n",
        "        for i in range(num_objs):\n",
        "            ann = {}\n",
        "            ann['image_id'] = image_id\n",
        "            ann['bbox'] = bboxes[i]\n",
        "            ann['category_id'] = labels[i]\n",
        "            categories.add(labels[i])\n",
        "            ann['area'] = areas[i]\n",
        "            ann['iscrowd'] = iscrowd[i]\n",
        "            ann['id'] = ann_id\n",
        "            if 'masks' in targets:\n",
        "                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n",
        "            if 'keypoints' in targets:\n",
        "                ann['keypoints'] = keypoints[i]\n",
        "                ann['num_keypoints'] = sum(k != 0 for k in keypoints[i][2::3])\n",
        "            dataset['annotations'].append(ann)\n",
        "            ann_id += 1\n",
        "    dataset['categories'] = [{'id': i} for i in sorted(categories)]\n",
        "    coco_ds.dataset = dataset\n",
        "    coco_ds.createIndex()\n",
        "    return coco_ds\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "            break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "    return convert_to_coco_api(dataset)\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms):\n",
        "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = dict(image_id=image_id, annotations=target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def get_coco(root, image_set, transforms, mode='instances'):\n",
        "    anno_file_template = \"{}_{}2017.json\"\n",
        "    PATHS = {\n",
        "        \"train\": (\"train2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"train\"))),\n",
        "        \"val\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\"))),\n",
        "        # \"train\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\")))\n",
        "    }\n",
        "\n",
        "    t = [ConvertCocoPolysToMask()]\n",
        "\n",
        "    if transforms is not None:\n",
        "        t.append(transforms)\n",
        "    transforms = T.Compose(t)\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    img_folder = os.path.join(root, img_folder)\n",
        "    ann_file = os.path.join(root, ann_file)\n",
        "\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=transforms)\n",
        "\n",
        "    if image_set == \"train\":\n",
        "        dataset = _coco_remove_images_without_annotations(dataset)\n",
        "\n",
        "    # dataset = torch.utils.data.Subset(dataset, [i for i in range(500)])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coco_kp(root, image_set, transforms):\n",
        "    return get_coco(root, image_set, transforms, mode=\"person_keypoints\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _flip_coco_person_keypoints(kps, width):\n",
        "    flip_inds = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n",
        "    flipped_data = kps[:, flip_inds]\n",
        "    flipped_data[..., 0] = width - flipped_data[..., 0]\n",
        "    # Maintain COCO convention that if visibility == 0, then x, y = 0\n",
        "    inds = flipped_data[..., 2] == 0\n",
        "    flipped_data[inds] = 0\n",
        "    return flipped_data\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = image.flip(-1)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "            target[\"boxes\"] = bbox\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = target[\"masks\"].flip(-1)\n",
        "            if \"keypoints\" in target:\n",
        "                keypoints = target[\"keypoints\"]\n",
        "                keypoints = _flip_coco_person_keypoints(keypoints, width)\n",
        "                target[\"keypoints\"] = keypoints\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
        "\n",
        "    def f(x):\n",
        "        if x >= warmup_iters:\n",
        "            return 1\n",
        "        alpha = float(x) / warmup_iters\n",
        "        return warmup_factor * (1 - alpha) + alpha\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as e:\n",
        "        if e.errno != errno.EEXIST:\n",
        "            raise\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "        args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n",
        "class CocoEvaluator(object):\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "            coco_dt = loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = evaluate_coco(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(\"IoU metric: {}\".format(iou_type))\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        elif iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        elif iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
        "                for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        'keypoints': keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # keep only unique (and in sorted order) images\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# From pycocotools, just removed the prints and fixed\n",
        "# a Python3 bug about unicode not defined\n",
        "#################################################################\n",
        "\n",
        "# Ideally, pycocotools wouldn't have hard-coded prints\n",
        "# so that we could avoid copy-pasting those two functions\n",
        "\n",
        "def createIndex(self):\n",
        "    # create index\n",
        "    # print('creating index...')\n",
        "    anns, cats, imgs = {}, {}, {}\n",
        "    imgToAnns, catToImgs = defaultdict(list), defaultdict(list)\n",
        "    if 'annotations' in self.dataset:\n",
        "        for ann in self.dataset['annotations']:\n",
        "            imgToAnns[ann['image_id']].append(ann)\n",
        "            anns[ann['id']] = ann\n",
        "\n",
        "    if 'images' in self.dataset:\n",
        "        for img in self.dataset['images']:\n",
        "            imgs[img['id']] = img\n",
        "\n",
        "    if 'categories' in self.dataset:\n",
        "        for cat in self.dataset['categories']:\n",
        "            cats[cat['id']] = cat\n",
        "\n",
        "    if 'annotations' in self.dataset and 'categories' in self.dataset:\n",
        "        for ann in self.dataset['annotations']:\n",
        "            catToImgs[ann['category_id']].append(ann['image_id'])\n",
        "\n",
        "    # print('index created!')\n",
        "\n",
        "    # create class members\n",
        "    self.anns = anns\n",
        "    self.imgToAnns = imgToAnns\n",
        "    self.catToImgs = catToImgs\n",
        "    self.imgs = imgs\n",
        "    self.cats = cats\n",
        "\n",
        "\n",
        "maskUtils = mask_util\n",
        "\n",
        "\n",
        "def loadRes(self, resFile):\n",
        "    \"\"\"\n",
        "    Load result file and return a result api object.\n",
        "    :param   resFile (str)     : file name of result file\n",
        "    :return: res (obj)         : result api object\n",
        "    \"\"\"\n",
        "    res = COCO()\n",
        "    res.dataset['images'] = [img for img in self.dataset['images']]\n",
        "\n",
        "    # print('Loading and preparing results...')\n",
        "    # tic = time.time()\n",
        "    if isinstance(resFile, torch._six.string_classes):\n",
        "        anns = json.load(open(resFile))\n",
        "    elif type(resFile) == np.ndarray:\n",
        "        anns = self.loadNumpyAnnotations(resFile)\n",
        "    else:\n",
        "        anns = resFile\n",
        "    assert type(anns) == list, 'results in not an array of objects'\n",
        "    annsImgIds = [ann['image_id'] for ann in anns]\n",
        "    assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n",
        "        'Results do not correspond to current coco set'\n",
        "    if 'caption' in anns[0]:\n",
        "        imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])\n",
        "        res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]\n",
        "        for id, ann in enumerate(anns):\n",
        "            ann['id'] = id + 1\n",
        "    elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n",
        "        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
        "        for id, ann in enumerate(anns):\n",
        "            bb = ann['bbox']\n",
        "            x1, x2, y1, y2 = [bb[0], bb[0] + bb[2], bb[1], bb[1] + bb[3]]\n",
        "            if 'segmentation' not in ann:\n",
        "                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n",
        "            ann['area'] = bb[2] * bb[3]\n",
        "            ann['id'] = id + 1\n",
        "            ann['iscrowd'] = 0\n",
        "    elif 'segmentation' in anns[0]:\n",
        "        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
        "        for id, ann in enumerate(anns):\n",
        "            # now only support compressed RLE format as segmentation results\n",
        "            ann['area'] = maskUtils.area(ann['segmentation'])\n",
        "            if 'bbox' not in ann:\n",
        "                ann['bbox'] = maskUtils.toBbox(ann['segmentation'])\n",
        "            ann['id'] = id + 1\n",
        "            ann['iscrowd'] = 0\n",
        "    elif 'keypoints' in anns[0]:\n",
        "        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
        "        for id, ann in enumerate(anns):\n",
        "            s = ann['keypoints']\n",
        "            x = s[0::3]\n",
        "            y = s[1::3]\n",
        "            x1, x2, y1, y2 = np.min(x), np.max(x), np.min(y), np.max(y)\n",
        "            ann['area'] = (x2 - x1) * (y2 - y1)\n",
        "            ann['id'] = id + 1\n",
        "            ann['bbox'] = [x1, y1, x2 - x1, y2 - y1]\n",
        "    # print('DONE (t={:0.2f}s)'.format(time.time()- tic))\n",
        "\n",
        "    res.dataset['annotations'] = anns\n",
        "    createIndex(res)\n",
        "    return res\n",
        "\n",
        "\n",
        "def evaluate_coco(self):\n",
        "    '''\n",
        "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
        "    :return: None\n",
        "    '''\n",
        "    # tic = time.time()\n",
        "    # print('Running per image evaluation...')\n",
        "    p = self.params\n",
        "    # add backward compatibility if useSegm is specified in params\n",
        "    if p.useSegm is not None:\n",
        "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
        "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
        "    # print('Evaluate annotation type *{}*'.format(p.iouType))\n",
        "    p.imgIds = list(np.unique(p.imgIds))\n",
        "    if p.useCats:\n",
        "        p.catIds = list(np.unique(p.catIds))\n",
        "    p.maxDets = sorted(p.maxDets)\n",
        "    self.params = p\n",
        "\n",
        "    self._prepare()\n",
        "    # loop through images, area range, max detection number\n",
        "    catIds = p.catIds if p.useCats else [-1]\n",
        "\n",
        "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
        "        computeIoU = self.computeIoU\n",
        "    elif p.iouType == 'keypoints':\n",
        "        computeIoU = self.computeOks\n",
        "    self.ious = {\n",
        "        (imgId, catId): computeIoU(imgId, catId)\n",
        "        for imgId in p.imgIds\n",
        "        for catId in catIds}\n",
        "\n",
        "    evaluateImg = self.evaluateImg\n",
        "    maxDet = p.maxDets[-1]\n",
        "    evalImgs = [\n",
        "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
        "        for catId in catIds\n",
        "        for areaRng in p.areaRng\n",
        "        for imgId in p.imgIds\n",
        "    ]\n",
        "    # this is NOT in the pycocotools code, but could be done outside\n",
        "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
        "    self._paramsEval = copy.deepcopy(self.params)\n",
        "    # toc = time.time()\n",
        "    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
        "    return p.imgIds, evalImgs\n",
        "\n",
        "#################################################################\n",
        "# end of straight copy from pycocotools, just removing the prints\n",
        "#################################################################"
      ],
      "metadata": {
        "id": "AmH_K89NNTRZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin importing libraries"
      ],
      "metadata": {
        "id": "57t-yJRAMJAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "\n",
        "import copy\n",
        "import torch.utils.data\n",
        "from pycocotools import mask as coco_mask\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import time\n",
        "import torch._six\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import pycocotools.mask as mask_util\n",
        "import json\n",
        "import tempfile\n",
        "\n",
        "from torchvision.transforms import functional as F\n",
        "import torch.distributed as dist\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "import errno\n",
        "import datetime\n",
        "import pickle\n",
        "import time\n",
        "from __future__ import print_function\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')\n",
        "print('You will train using your ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j8_CdBaMIVq",
        "outputId": "5f4e286c-834e-4818-99ba-d705aedab3e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You will train using your  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceMask_Dataset(object):\n",
        "    def __init__(self, root, obj, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.img_path = os.path.join(root, 'imgs')\n",
        "        self.label_path = os.path.join(root, 'new_labels')\n",
        "        \n",
        "        # Read labels\n",
        "        self.labels = list(sorted(os.listdir(self.label_path)))\n",
        "        label_names = [x[:-4] for x in self.labels]\n",
        "        \n",
        "        #Read Images\n",
        "        self.imgs = list(sorted(os.listdir(self.img_path)))\n",
        "        \n",
        "        #Make both lists of images and labels match to avoid errors\n",
        "        self.imgs = [x for x in self.imgs if x[:-4] in label_names]\n",
        "        img_names = [x[:-4] for x in self.imgs]\n",
        "        self.labels = [x for x in self.labels if x[:-4] in img_names]\\\n",
        "        \n",
        "        if obj=='train':\n",
        "            self.imgs = self.imgs[2000:]   # take all images except first 2k\n",
        "            self.labels = self.labels[2000:]\n",
        "        elif obj=='test':\n",
        "            self.imgs = self.imgs[:2000]   # take only the first 2k images\n",
        "            self.labels = self.labels[:2000]\n",
        "        \n",
        "\n",
        "    #Generates the bounding box based on the coordinates given\n",
        "    def create_box_from_coordinates(self, coordinates):\n",
        "        box=[int(coordinates[0][1:]),\n",
        "             int(coordinates[1]),\n",
        "             int(coordinates[2]),\n",
        "             int(coordinates[3][:-1])]\n",
        "        return [box]  # returns bounding box of shape = [[x_min, y_min, x_max, y_max]] --> nested list (list inside list)\n",
        "    \n",
        "\n",
        "    #Extracts the label and bounding box\n",
        "    def read_label(self, path):\n",
        "        #Read the txt file with the bounding boxes information\n",
        "        file_object  = open(path, \"r\")\n",
        "        data = file_object.readlines()\n",
        "        data = data[0].split(',')[-6:-1]\n",
        "        \n",
        "        label, boxes = data[0], data[1:]\n",
        "        label = int(label)\n",
        "        if label == 0: #if NOT wearing mask, change to label 3\n",
        "            label = 3\n",
        "        box = self.create_box_from_coordinates(boxes)\n",
        "        return [label], box  # --> returns: [label], bounding box\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):  # This function should read the image, and return both the image and label as tensors!!!\n",
        "#         print('idx: ', idx)\n",
        "        \n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.img_path, self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        label_path = os.path.join(self.label_path, self.labels[idx])\n",
        "        labels, boxes = self.read_label(label_path)\n",
        "        \n",
        "        #Calculate number of objects in image\n",
        "        n_objects=1\n",
        "        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])  # --> just the number of the image. \n",
        "        \n",
        "        #Calculate bounding box area\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        # We put the label information as a target --> because this is how fasterRCNN model expects it!\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "yZ1JQR8PLwi4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define function that converts pillow images into tensors"
      ],
      "metadata": {
        "id": "F9tq8Vr6O6Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms a pillow image to tensor and applies some augmentation\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(ToTensor())\n",
        "    if train:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "    return Compose(transforms)"
      ],
      "metadata": {
        "id": "OFhJgqw9OzNf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster RCNN Parameters!!!"
      ],
      "metadata": {
        "id": "Xf7bsafUPKAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probability threshold for your Non-Maxima Supression --> to remove some of the false predictions\n",
        "box_nms_thresh = 0.5\n",
        "\n",
        "# 0 = No Person\n",
        "# 1 = Not wearing mask CORRECTLY\n",
        "# 2 = Wearing mask =)\n",
        "# 3 = NOT wearing mask! (>_<)\n",
        "num_classes = 4  \n",
        "\n",
        "num_epochs = 1 #Number of epochs to train"
      ],
      "metadata": {
        "id": "bagNa-gXPHf9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare your faster-RCNN backbone (pre-trained network)"
      ],
      "metadata": {
        "id": "0iKjGJz1Pl_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your must first load the previously trained FasterRCNN model from Pytorch\n",
        "# first in order to fine-tune it. \n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
        "                                                pretrained=True, \n",
        "                                                box_nms_thresh=box_nms_thresh\n",
        "                                                )\n",
        "\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)   \n",
        "\n",
        "print(model.roi_heads.box_predictor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "9ef3291b55d44b13b6e03c2ce4885818",
            "93104761bb5c47c69318c9ed4c576d10",
            "119e5bce567f4b1c8a7f579b60c3cfb9",
            "e9fa9f6af2cd40148c506be7d46c082d",
            "0880de47887a4245ac374243b2b56a5e",
            "fbfad41e268e4b95bed2e1ad3c3c0386",
            "cb4eb445404e41a88de930b081c780a2",
            "a38e7a8c2af247ce8b0b93b21cb4e151",
            "11abce6e7ee64c898cf0357fd194522f",
            "436d0ab5e4774b04a5a1b9fb7a55eafd",
            "a8fc4784ba6544348d10b2678a08bc90"
          ]
        },
        "id": "ogZj86x0POAb",
        "outputId": "9fe82694-c344-4e49-a554-d918b953db1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ef3291b55d44b13b6e03c2ce4885818"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastRCNNPredictor(\n",
            "  (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
            "  (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize your Train and Test Dataloaders!"
      ],
      "metadata": {
        "id": "CeI7dnM3QxXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = dataset = FaceMask_Dataset(\n",
        "                                    \"/content/sample_data/MaskDataset__ByTomasMendoza\",  # Root Folder\n",
        "                                    \"train\", \n",
        "                                    get_transform(train=True)) \n",
        "\n",
        "dataset_test =dataset = FaceMask_Dataset(\n",
        "                                    \"/content/sample_data/MaskDataset__ByTomasMendoza\",  # Root folder\n",
        "                                    \"test\", \n",
        "                                    get_transform(train=False))\n",
        "\n",
        "\n",
        "# define training and testing data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "                                        dataset, \n",
        "                                        batch_size=2, \n",
        "                                        shuffle=True, \n",
        "                                        num_workers=0,\n",
        "                                        collate_fn=collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "                                        dataset_test, \n",
        "                                        batch_size=1, \n",
        "                                        shuffle=False, \n",
        "                                        num_workers=0,\n",
        "                                        collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "WXb-KpyYQrl1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the output of your dataloader!"
      ],
      "metadata": {
        "id": "GINMVLI8oVNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize only the first image and first target since\n",
        "# dataloader returns batch of images and targets and we only\n",
        "# need to make sure it works\n",
        "number_of_batches_to_test = 2\n",
        "for idx,(image, target) in enumerate(data_loader):\n",
        "  print(\"####################################\")\n",
        "  print(\"\\nBatch = \", idx)\n",
        "  print(\"Image 0 = \", image[0])\n",
        "  print(\"\\nImage 0 has a shape = \", image[0].shape)\n",
        "  print(\"\\nTarget 0 = \", target[0])\n",
        "  if idx >= number_of_batches_to_test-1:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ0a8aoqoVWo",
        "outputId": "d5db9be1-ac14-4366-93c0-d040234529f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################################\n",
            "\n",
            "Batch =  0\n",
            "Image 0 =  tensor([[[0.8157, 0.8196, 0.8235,  ..., 0.8000, 0.7647, 0.7333],\n",
            "         [0.8196, 0.8196, 0.8196,  ..., 0.8078, 0.7725, 0.7608],\n",
            "         [0.8235, 0.8196, 0.8196,  ..., 0.8078, 0.7725, 0.7686],\n",
            "         ...,\n",
            "         [0.7373, 0.7451, 0.7333,  ..., 0.8078, 0.8078, 0.8078],\n",
            "         [0.7608, 0.7373, 0.7373,  ..., 0.8157, 0.8118, 0.8118],\n",
            "         [0.7647, 0.7412, 0.7373,  ..., 0.8157, 0.8118, 0.8118]],\n",
            "\n",
            "        [[0.7922, 0.7961, 0.8000,  ..., 0.8000, 0.7686, 0.7333],\n",
            "         [0.7961, 0.7961, 0.7961,  ..., 0.8078, 0.7725, 0.7569],\n",
            "         [0.8000, 0.7961, 0.7961,  ..., 0.8078, 0.7725, 0.7647],\n",
            "         ...,\n",
            "         [0.7216, 0.7255, 0.7137,  ..., 0.8078, 0.8078, 0.8078],\n",
            "         [0.7608, 0.7333, 0.7333,  ..., 0.8157, 0.8118, 0.8118],\n",
            "         [0.7647, 0.7373, 0.7333,  ..., 0.8157, 0.8118, 0.8118]],\n",
            "\n",
            "        [[0.7451, 0.7490, 0.7529,  ..., 0.7608, 0.7137, 0.7725],\n",
            "         [0.7490, 0.7490, 0.7490,  ..., 0.7686, 0.7255, 0.7765],\n",
            "         [0.7529, 0.7490, 0.7490,  ..., 0.7765, 0.7333, 0.7569],\n",
            "         ...,\n",
            "         [0.7098, 0.7098, 0.6902,  ..., 0.8000, 0.8000, 0.8000],\n",
            "         [0.7294, 0.7137, 0.7176,  ..., 0.8078, 0.8039, 0.8039],\n",
            "         [0.7333, 0.7176, 0.7176,  ..., 0.8078, 0.8039, 0.8039]]])\n",
            "\n",
            "Image 0 has a shape =  torch.Size([3, 458, 305])\n",
            "\n",
            "Target 0 =  {'boxes': tensor([[195.,  59., 239., 103.]]), 'labels': tensor([2]), 'image_id': tensor([321]), 'area': tensor([1936.]), 'iscrowd': tensor([0])}\n",
            "####################################\n",
            "\n",
            "Batch =  1\n",
            "Image 0 =  tensor([[[0.9804, 0.9804, 0.9804,  ..., 0.5765, 0.5569, 0.5451],\n",
            "         [0.9804, 0.9804, 0.9804,  ..., 0.5686, 0.5490, 0.5373],\n",
            "         [0.9804, 0.9804, 0.9804,  ..., 0.5569, 0.5373, 0.5255],\n",
            "         ...,\n",
            "         [0.9961, 0.9961, 0.9961,  ..., 0.4392, 0.4196, 0.4000],\n",
            "         [0.9961, 0.9961, 0.9961,  ..., 0.4353, 0.4118, 0.3922],\n",
            "         [0.9961, 0.9961, 0.9961,  ..., 0.4353, 0.4157, 0.3961]],\n",
            "\n",
            "        [[0.9647, 0.9647, 0.9647,  ..., 0.5765, 0.5569, 0.5451],\n",
            "         [0.9647, 0.9647, 0.9647,  ..., 0.5686, 0.5490, 0.5373],\n",
            "         [0.9647, 0.9647, 0.9647,  ..., 0.5569, 0.5373, 0.5255],\n",
            "         ...,\n",
            "         [0.9765, 0.9765, 0.9765,  ..., 0.6039, 0.5843, 0.5647],\n",
            "         [0.9765, 0.9765, 0.9765,  ..., 0.6000, 0.5765, 0.5569],\n",
            "         [0.9765, 0.9765, 0.9765,  ..., 0.6000, 0.5804, 0.5608]],\n",
            "\n",
            "        [[0.9176, 0.9176, 0.9176,  ..., 0.5451, 0.5255, 0.5137],\n",
            "         [0.9176, 0.9176, 0.9176,  ..., 0.5373, 0.5176, 0.5059],\n",
            "         [0.9176, 0.9176, 0.9176,  ..., 0.5255, 0.5059, 0.4941],\n",
            "         ...,\n",
            "         [0.9608, 0.9608, 0.9608,  ..., 0.6980, 0.6706, 0.6510],\n",
            "         [0.9608, 0.9608, 0.9608,  ..., 0.6941, 0.6627, 0.6431],\n",
            "         [0.9608, 0.9608, 0.9608,  ..., 0.6941, 0.6667, 0.6471]]])\n",
            "\n",
            "Image 0 has a shape =  torch.Size([3, 1071, 600])\n",
            "\n",
            "Target 0 =  {'boxes': tensor([[ 220.,  872.,  417., 1069.]]), 'labels': tensor([2]), 'image_id': tensor([1219]), 'area': tensor([38809.]), 'iscrowd': tensor([0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Optimizer + Loss Function"
      ],
      "metadata": {
        "id": "7IMvXG82SB94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "#Define a Stochastic Gradient Descent Optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "#Define optimizer for training\n",
        "optimizer = torch.optim.SGD(params, \n",
        "                            lr=0.005,            #Learning rate\n",
        "                            momentum=0.9,        #Momentum\n",
        "                            weight_decay=0.0005) #Learning Rate Decay\n",
        "\n",
        "#Initialize Learning Rate Scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,   #Optimizer --> SGD\n",
        "                                               step_size=3, #LR scheduler Step Size\n",
        "                                               gamma=0.1)   #Gamma discount factor"
      ],
      "metadata": {
        "id": "1BNnoF72RJ3F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Train Function"
      ],
      "metadata": {
        "id": "T1GMESs_TiNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "    model.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1. / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])"
      ],
      "metadata": {
        "id": "jCBNwvAFSHfW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Evaluate method"
      ],
      "metadata": {
        "id": "mcy21F2RTrir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "  n_threads = torch.get_num_threads()\n",
        "  # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "  torch.set_num_threads(1)\n",
        "  cpu_device = torch.device(\"cpu\")\n",
        "  model.eval()\n",
        "  metric_logger = MetricLogger(delimiter=\"  \")\n",
        "  header = 'Test:'\n",
        "\n",
        "  coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "  iou_types = _get_iou_types(model)\n",
        "  coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "  for image, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "    image = list(img.to(device) for img in image)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    model_time = time.time()\n",
        "    outputs = model(image)\n",
        "\n",
        "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "    model_time = time.time() - model_time\n",
        "\n",
        "    res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "    evaluator_time = time.time()\n",
        "    coco_evaluator.update(res)\n",
        "    evaluator_time = time.time() - evaluator_time\n",
        "    metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "  # gather the stats from all processes\n",
        "  metric_logger.synchronize_between_processes()\n",
        "  print(\"Averaged stats:\", metric_logger)\n",
        "  coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "  # accumulate predictions from all images\n",
        "  coco_evaluator.accumulate()\n",
        "  coco_evaluator.summarize()\n",
        "  torch.set_num_threads(n_threads)\n",
        "  return coco_evaluator"
      ],
      "metadata": {
        "id": "0UlkDjjFTru0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin Training"
      ],
      "metadata": {
        "id": "yWdkuieATnkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print('epoch: ', epoch)\n",
        "    # train for one epoch, printing every (print_freq) iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model=model, data_loader=data_loader_test, device=device)\n",
        "    \n",
        "    break\n",
        "\n",
        "print(\"Niceee! You're done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcIB1dJpTb37",
        "outputId": "1f431e91-2896-4b4e-eb4d-125dfbda16e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "Epoch: [0]  [   0/1250]  eta: 2:07:25  lr: 0.000010  loss: 1.7700 (1.7700)  loss_classifier: 1.6601 (1.6601)  loss_box_reg: 0.0633 (0.0633)  loss_objectness: 0.0399 (0.0399)  loss_rpn_box_reg: 0.0067 (0.0067)  time: 6.1162  data: 0.1033  max mem: 6592\n",
            "Epoch: [0]  [  50/1250]  eta: 1:32:32  lr: 0.000260  loss: 0.3375 (0.7348)  loss_classifier: 0.1746 (0.5799)  loss_box_reg: 0.1365 (0.1199)  loss_objectness: 0.0133 (0.0258)  loss_rpn_box_reg: 0.0076 (0.0091)  time: 4.3521  data: 0.1441  max mem: 8971\n",
            "Epoch: [0]  [ 100/1250]  eta: 1:26:26  lr: 0.000509  loss: 0.2686 (0.5063)  loss_classifier: 0.1046 (0.3515)  loss_box_reg: 0.1417 (0.1269)  loss_objectness: 0.0109 (0.0200)  loss_rpn_box_reg: 0.0057 (0.0078)  time: 4.2606  data: 0.0764  max mem: 9356\n",
            "Epoch: [0]  [ 150/1250]  eta: 1:23:26  lr: 0.000759  loss: 0.2417 (0.4185)  loss_classifier: 0.0819 (0.2647)  loss_box_reg: 0.1390 (0.1307)  loss_objectness: 0.0073 (0.0162)  loss_rpn_box_reg: 0.0045 (0.0069)  time: 4.6076  data: 0.0823  max mem: 9356\n",
            "Epoch: [0]  [ 200/1250]  eta: 1:18:57  lr: 0.001009  loss: 0.1790 (0.3666)  loss_classifier: 0.0682 (0.2177)  loss_box_reg: 0.1053 (0.1289)  loss_objectness: 0.0034 (0.0137)  loss_rpn_box_reg: 0.0035 (0.0063)  time: 4.7593  data: 0.0810  max mem: 9356\n",
            "Epoch: [0]  [ 250/1250]  eta: 1:15:15  lr: 0.001259  loss: 0.1548 (0.3301)  loss_classifier: 0.0687 (0.1887)  loss_box_reg: 0.0864 (0.1236)  loss_objectness: 0.0022 (0.0122)  loss_rpn_box_reg: 0.0024 (0.0057)  time: 4.8056  data: 0.1091  max mem: 9356\n",
            "Epoch: [0]  [ 300/1250]  eta: 1:10:56  lr: 0.001508  loss: 0.1622 (0.3054)  loss_classifier: 0.0631 (0.1696)  loss_box_reg: 0.0940 (0.1196)  loss_objectness: 0.0006 (0.0110)  loss_rpn_box_reg: 0.0022 (0.0052)  time: 4.1398  data: 0.0869  max mem: 9356\n",
            "Epoch: [0]  [ 350/1250]  eta: 1:06:43  lr: 0.001758  loss: 0.1372 (0.2853)  loss_classifier: 0.0508 (0.1549)  loss_box_reg: 0.0883 (0.1159)  loss_objectness: 0.0002 (0.0098)  loss_rpn_box_reg: 0.0016 (0.0048)  time: 4.6490  data: 0.0657  max mem: 9356\n",
            "Epoch: [0]  [ 400/1250]  eta: 1:03:30  lr: 0.002008  loss: 0.1656 (0.2696)  loss_classifier: 0.0699 (0.1436)  loss_box_reg: 0.0878 (0.1126)  loss_objectness: 0.0007 (0.0090)  loss_rpn_box_reg: 0.0018 (0.0045)  time: 4.6639  data: 0.0540  max mem: 9356\n",
            "Epoch: [0]  [ 450/1250]  eta: 0:59:52  lr: 0.002258  loss: 0.1416 (0.2563)  loss_classifier: 0.0584 (0.1344)  loss_box_reg: 0.0852 (0.1094)  loss_objectness: 0.0003 (0.0082)  loss_rpn_box_reg: 0.0017 (0.0043)  time: 4.9164  data: 0.0973  max mem: 9356\n",
            "Epoch: [0]  [ 500/1250]  eta: 0:55:55  lr: 0.002507  loss: 0.1477 (0.2453)  loss_classifier: 0.0568 (0.1268)  loss_box_reg: 0.0906 (0.1067)  loss_objectness: 0.0004 (0.0077)  loss_rpn_box_reg: 0.0017 (0.0041)  time: 4.3645  data: 0.0592  max mem: 9356\n",
            "Epoch: [0]  [ 550/1250]  eta: 0:52:11  lr: 0.002757  loss: 0.1542 (0.2363)  loss_classifier: 0.0555 (0.1208)  loss_box_reg: 0.0854 (0.1044)  loss_objectness: 0.0005 (0.0072)  loss_rpn_box_reg: 0.0017 (0.0039)  time: 4.4348  data: 0.0880  max mem: 9356\n",
            "Epoch: [0]  [ 600/1250]  eta: 0:48:33  lr: 0.003007  loss: 0.1335 (0.2284)  loss_classifier: 0.0589 (0.1158)  loss_box_reg: 0.0694 (0.1021)  loss_objectness: 0.0005 (0.0068)  loss_rpn_box_reg: 0.0016 (0.0037)  time: 4.7045  data: 0.0617  max mem: 9356\n",
            "Epoch: [0]  [ 650/1250]  eta: 0:44:55  lr: 0.003257  loss: 0.1397 (0.2218)  loss_classifier: 0.0596 (0.1114)  loss_box_reg: 0.0751 (0.1004)  loss_objectness: 0.0002 (0.0064)  loss_rpn_box_reg: 0.0014 (0.0036)  time: 4.5181  data: 0.0807  max mem: 9356\n",
            "Epoch: [0]  [ 700/1250]  eta: 0:41:15  lr: 0.003506  loss: 0.1180 (0.2153)  loss_classifier: 0.0443 (0.1074)  loss_box_reg: 0.0676 (0.0984)  loss_objectness: 0.0003 (0.0060)  loss_rpn_box_reg: 0.0012 (0.0035)  time: 4.5352  data: 0.1662  max mem: 9356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import gmtime, strftime\n",
        "time_now = strftime(\"%d_%m_%Y_%H_%M\", gmtime())\n",
        "torch.save(model, 'entire_facemask_dection_model.pth')\n",
        "torch.save(model.state_dict(), 'weights_facemask_epochs{}_date_{}.pth'.format(num_epochs,time_now))"
      ],
      "metadata": {
        "id": "zpyCEalTTb6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "box_nms_thresh=0.5\n",
        "\n",
        "model = torch.load('entire_facemask_dection_model.pth')\n",
        "print(model)"
      ],
      "metadata": {
        "id": "YSlTOewXA0pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def display_results(boxes, labels, img_path, threshold=0.5, rect_th=3, text_size=0.8, text_th=2):\n",
        "    print(img_path)\n",
        "    img = cv2.imread(img_path) # Read image with cv2\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
        "    for i in range(len(boxes)):\n",
        "        #not wearing mask at all\n",
        "        if labels[i]==3: \n",
        "            color_to_use=(255,0,0)\n",
        "            labels[i]=\"Not wearing mask! >:\"\n",
        "\n",
        "        #Incorrectly wearing mask\n",
        "        elif labels[i]==1: \n",
        "            color_to_use=(255,255,0)\n",
        "            labels[i]=\"Incorrect\"\n",
        "\n",
        "        elif labels[i]==2: #wearing mask\n",
        "            color_to_use=(0,255,0)\n",
        "            labels[i]=\"Wearing mask!\"\n",
        "        start = (boxes[i][0], boxes[i][1])\n",
        "        finish = (boxes[i][2], boxes[i][3])\n",
        "        cv2.rectangle(img, start, finish, color=color_to_use, thickness=rect_th) # Draw Rectangle with the coordinates\n",
        "        #cv2.putText(img, str(labels[i]), start,  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) # Write the prediction class\n",
        "    plt.figure(figsize=(9,15)) # display the output image\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NXlqccP_A5Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "  \n",
        "for i in range (110,115):\n",
        "    img = \"test_00000{}.jpg\".format(i)\n",
        "    path = os.path.join('/content/sample_data/MaskDataset__ByTomasMendoza','test',img)\n",
        "    img_ = Image.open(path).convert('RGB')\n",
        "    tran = T.ToTensor()\n",
        "    model.eval()\n",
        "    preds = model([tran(img_).cuda()])\n",
        "    boxes, scores, labels = preds[0]['boxes'], preds[0]['scores'], preds[0]['labels']\n",
        "\n",
        "    #Use NMS to find overlapping boxes\n",
        "    results=torchvision.ops.nms(boxes, scores, 0.2)\n",
        "\n",
        "    #transform to list\n",
        "    results=list(results.data.cpu().numpy())\n",
        "    boxes=list(boxes.data.cpu().numpy())\n",
        "    labels=list(labels.data.cpu().numpy())\n",
        "\n",
        "    #Remove overlapping boxes\n",
        "    boxes=[boxes[x] for x in results]\n",
        "    labels=[labels[x] for x in results]\n",
        "    \n",
        "    print('img')\n",
        "\n",
        "    display_results(boxes, labels, path)"
      ],
      "metadata": {
        "id": "-YHB_S6TA9AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lAAJxsgvBN2R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}